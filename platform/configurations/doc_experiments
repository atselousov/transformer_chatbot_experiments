N_EPOCHS=3		Number of epoch
TRAIN_BATCH_SIZE=256	total batch size													=> ?? we could try a smaller value maybe
BATCH_SPLIT=64			divide the batch in split (and summ gradients)
TEST_BATCH_SIZE=8		batch size for testing
S2S_WEIGHT=1			weights of auxiliary loss on encoded inputs (for dual input model only)
LM_WEIGHT=0.5			standard LM loss on output
RISK_WEIGHT=0			risk loss using F1 metrics											=> we can use other metrics also now
HITS_WEIGHT=1			loss on hits@1 classification (using multi_choice_head on the model)
NEGATIVE_SAMPLES=3		negative samples for the hits@1 loss
SINGLE_INPUT=True		use a single input model (HuggingFace-like but still missing constraints on positional encoding)
DIALOG_EMBEDDINGS=True	use additional dialog embeddings
USE_START_END=True		use start and end tokens (ex: <t1>, </t1> surrounding each part)	=> could be removed for single-input model w. dialog embeddings to avoid copying these
LABEL_SMOOTHING=0.1		label_smoothing factor
PERSONA_AUGMENT=False	augment person sentences
PERSONA_AUG_SYN_PROBA=0.0	factor for person augmentation
FP16=True				use fp16 training
LOSS_SCALE=0			loss scaling for fp16 training 0 => dynamic adjustement
LINEAR_SCHEDULE=True	use linear schedule for the learning rate (increase for warmup ratio and decrease until zero at the end)
MULTIPLE_CHOICE_HEAD=False	set to Tru to use hits@1 classification and negative samples
BEAM_SIZE=3				beam size
DIVERSITY_COEF=0		beam search parameters
DIVERSITY_GROUP=1
ANNEALING=0
ANNEALING_TOPK=None
EVALUATE_FULL_SEQUENCES=True	set to true to perform evaluation (at the end of each epoch) on full sentence metrics also: F1 and external metrics. set to False to evaluate faster
LIMIT_EVAL_TIME=-1				if evaluation is too long (ex: using full sentence metrics with the single-input model), we can limit to a subset of the evaluation set here